{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import category_encoders as ce\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import set_config\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV,ParameterGrid\n",
    "import optuna.integration.lightgbm as lgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df = pd.read_csv('application_train.csv')\n",
    "test_df = pd.read_csv('application_test.csv')\n",
    "bureau_balance = pd.read_csv('bureau_balance.csv')\n",
    "bureau = pd.read_csv('bureau.csv')\n",
    "credit_card_balance = pd.read_csv('credit_card_balance.csv')\n",
    "HomeCredit_columns_description = pd.read_csv('HomeCredit_columns_description.csv')\n",
    "installments_payments =  pd.read_csv('installments_payments.csv')\n",
    "POS_CASH_balance = pd.read_csv('POS_CASH_balance.csv')\n",
    "previous_application = pd.read_csv('previous_application.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df.isnull().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def missing_value_table(df):\n",
    "    #total missing value\n",
    "    mis_val = df.isnull().sum()\n",
    "\n",
    "    #percentage of missing value\n",
    "    mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "\n",
    "    #make a table with the results\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent],axis=1)\n",
    "\n",
    "    #rename the columns \n",
    "    mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns={0 : 'missing values',1 : '% of total values'}\n",
    "    )\n",
    "    #sort the table by percentage of missing decsending\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1] != 0].sort_values('% of total values',ascending=False).round(1)\n",
    "\n",
    "    #print summary information\n",
    "    print('Your selected dataframe has ' + str(df.shape[1]) + ' columns.\\n'\n",
    "          'There are ' + str(mis_val_table_ren_columns.shape[0]) + \n",
    "          ' columns that have missing values'\n",
    "    )\n",
    "\n",
    "    #return the dataframe with missing information\n",
    "    return mis_val_table_ren_columns\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "missing_value_table(train_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df.select_dtypes('object').apply(pd.Series.nunique,axis=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "le = LabelEncoder()\n",
    "le_count = 0\n",
    "\n",
    "for col in train_df:\n",
    "    if train_df[col].dtype == 'object':\n",
    "        if len(list(train_df[col].unique())) <= 2:\n",
    "\n",
    "            le.fit(train_df[col])\n",
    "            train_df[col] = le.transform(train_df[col])\n",
    "            test_df[col] = le.transform(test_df[col])\n",
    "\n",
    "            le_count += 1\n",
    "\n",
    "print('%d columns were label encoded' %le_count)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df = pd.get_dummies(train_df)\n",
    "test_df = pd.get_dummies(test_df)\n",
    "\n",
    "print('Training Feature shape: ', train_df.shape)\n",
    "print('Test Feature shape: ', test_df.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_labels = train_df['TARGET']\n",
    "\n",
    "train_df, test_df = train_df.align(test_df, join='inner',axis=1)\n",
    "\n",
    "train_df['TARGET'] = train_labels\n",
    "\n",
    "print('Training Feature Shape: ', train_df.shape)\n",
    "print('Test Feature Shape: ', test_df.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "(train_df['DAYS_BIRTH'] / -365).describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df['DAYS_EMPLOYED'].describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df['DAYS_EMPLOYED'].plot.hist(title='Days Employed Histogram')\n",
    "plt.xlabel('Days Employment')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#create an anomalous flag column\n",
    "train_df['DAYS_EMPLOYED_ANOM'] = train_df['DAYS_EMPLOYED'] == 365243\n",
    "\n",
    "#Replace the anomalous values with nan\n",
    "train_df['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace=True)\n",
    "\n",
    "train_df['DAYS_EMPLOYED'].plot.hist(title='Days Employment Histogram')\n",
    "plt.xlabel('Days Employment')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_df['DAYS_EMPLOYED_ANOM'] = test_df['DAYS_EMPLOYED'] == 365243\n",
    "test_df['DAYS_EMPLOYED'].replace({365243:np.nan},inplace=True)\n",
    "\n",
    "print('There are %d anomalies in the test data out of %d entries' % (test_df['DAYS_EMPLOYED_ANOM'].sum(),len(test_df)))  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Find correlations with the target and sort\n",
    "correlations = train_df.corr()['TARGET'].sort_values()\n",
    "\n",
    "#Display correlations\n",
    "print('Most Positive Correlations:\\n',correlations.tail(15))\n",
    "print('\\nMost Negative Correlaitons:\\n',correlations.head(15))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Find the correlation of the positive days since birth and target\n",
    "\n",
    "train_df['DAYS_BIRTH'] = abs(train_df['DAYS_BIRTH'])\n",
    "train_df['DAYS_BIRTH'].corr(train_df['TARGET'])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#set the style of plots\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "#plot the distribution of ages in years \n",
    "plt.hist(train_df['DAYS_BIRTH']/365, edgecolor='k', bins=25)\n",
    "plt.title('Age of Client'); plt.xlabel('Age(years)'); plt.ylabel('Count'); \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "#KDE plot of loans that were repaid on time \n",
    "sns.kdeplot(train_df.loc[train_df['TARGET'] == 0, 'DAYS_BIRTH'] /365,label='target = 0')\n",
    "\n",
    "#KDE plot of loans that were not repaid on time\n",
    "sns.kdeplot(train_df.loc[train_df['TARGET'] == 1, 'DAYS_BIRTH']/365,label='target = 1')\n",
    "\n",
    "# Labeling of plot\n",
    "plt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Age information into a separate dataframe\n",
    "age_data = train_df[['TARGET','DAYS_BIRTH']]\n",
    "age_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH']/365\n",
    "\n",
    "#Bin the age data\n",
    "age_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'],bins=np.linspace(20,70,num=11))\n",
    "age_data.head(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Group by the bin and calculate averages\n",
    "age_groups = age_data.groupby('YEARS_BINNED').mean()\n",
    "age_groups"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "#graph the age bins and the average of the target as a bar plot\n",
    "plt.bar(age_groups.index.astype(str),100*age_groups['TARGET'])\n",
    "\n",
    "#plot labeling\n",
    "plt.xticks(rotation=75)\n",
    "plt.xlabel('Age Group(years)')\n",
    "plt.ylabel('Failure to repay(%)')\n",
    "plt.title('Failure to repay by age group')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Extract the EXT_SOURCE variables and show correlations\n",
    "ext_data = train_df[['TARGET','EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']]\n",
    "ext_data_corrs = ext_data.corr()\n",
    "ext_data_corrs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\n",
    "plt.title('Correlation Heatmap');\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10,12))\n",
    "\n",
    "#iterate through the sources\n",
    "for i, source in enumerate(['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3']):\n",
    "    #create a new subplot for each sources\n",
    "    plt.subplot(3,1,i+1)\n",
    "    #plot repaid loans\n",
    "    sns.kdeplot(train_df.loc[train_df['TARGET'] == 0, source], label = 'target=0')\n",
    "    #plot loans that were not repaid\n",
    "    sns.kdeplot(train_df.loc[train_df['TARGET'] == 1, source], label='target = 1')\n",
    "\n",
    "    #label the plots\n",
    "    plt.title('Distribution of %s by Target Value'%source)\n",
    "    plt.xlabel('%s'%source)\n",
    "    plt.ylabel('Density')\n",
    "\n",
    "\n",
    "plt.tight_layout(h_pad = 2.5)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#copy the data for plotting\n",
    "plot_data = ext_data.drop(columns=['DAYS_BIRTH']).copy()\n",
    "\n",
    "#Add in the age of client in years\n",
    "plot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n",
    "\n",
    "#Drop na values and limit to first 100000 rows \n",
    "plot_data = plot_data.dropna().loc[:100000, :]\n",
    "\n",
    "#Function to calculate correlation coefficient between two values\n",
    "def corr_func(x, y, **kwargs):\n",
    "    r = np.corrcoef(x, y)[0][1]\n",
    "    ax = plt.gca()\n",
    "    ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                xy=(.2, .8), xycoords=ax.transAxes,\n",
    "                size = 20)\n",
    "\n",
    "#Create the pairgrid object \n",
    "grid = sns.PairGrid(data = plot_data, height = 3, diag_sharey=False,\n",
    "                    hue = 'TARGET', \n",
    "                    vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n",
    "\n",
    "#upper is a scatter plot                \n",
    "grid.map_upper(plt.scatter, alpha = 0.2)\n",
    "\n",
    "# Bottom is density plot\n",
    "grid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n",
    "\n",
    "plt.suptitle('Ext Source and Age Features Pairs Plot', height = 32, y = 1.05);\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Make a new dataframe for polynomial features\n",
    "poly_features = train_df[['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH','TARGET']]\n",
    "\n",
    "poly_features_test = test_df[['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']]\n",
    "\n",
    "#imputer for handling missing value\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "poly_target = poly_features['TARGET']\n",
    "\n",
    "poly_features = poly_features.drop(columns=['TARGET'])\n",
    "\n",
    "#Need to impute missing values\n",
    "poly_features = imputer.fit_transform(poly_features)\n",
    "\n",
    "poly_features_test = imputer.transform(poly_features_test)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#Create the polynomial object with specified degree\n",
    "poly_transformer =  PolynomialFeatures(degree = 3)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#train the polynomial feature \n",
    "poly_transformer.fit(poly_features)\n",
    "\n",
    "#Transform the feature\n",
    "poly_features = poly_transformer.transform(poly_features)\n",
    "poly_features_test = poly_transformer.transform(poly_features_test)\n",
    "\n",
    "print('Polynomial Features Shape: ', poly_features.shape)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "poly_transformer.get_feature_names(input_features=['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH'])[:15]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Create a dataframe of the features\n",
    "poly_features = pd.DataFrame(poly_features,\n",
    "                             columns = poly_transformer.get_feature_names([\n",
    "                                 'EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH'\n",
    "                             ])\n",
    ")\n",
    "\n",
    "#Add in the target\n",
    "poly_features['TARGET'] = poly_target\n",
    "\n",
    "#Find the correlation with the target\n",
    "poly_corrs = poly_features.corr()['TARGET'].sort_values()\n",
    "\n",
    "#Display most negetive and most positive\n",
    "print(poly_corrs.head(10))\n",
    "print(poly_corrs.tail(5))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Put test features into dataframe\n",
    "poly_features_test = pd.DataFrame(poly_features_test, \n",
    "                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n",
    "                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']\n",
    "))\n",
    "\n",
    "#Merge polynomial feature into training dataframe\n",
    "poly_features['SK_ID_CURR'] = train_df['SK_ID_CURR']\n",
    "train_df_poly = train_df.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "# Merge polynomial features into testing dataframe\n",
    "poly_features_test['SK_ID_CURR'] = test_df['SK_ID_CURR']\n",
    "test_df_poly = test_df.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "#Align the dataframe\n",
    "train_df_poly,test_df_poly = train_df_poly.align(test_df_poly,join='inner',axis=1)\n",
    "\n",
    "# Print out the new shapes\n",
    "print('Training data with polynomial features shape: ', train_df_poly.shape)\n",
    "print('Testing data with polynomial features shape:  ', test_df_poly.shape)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df_domain = train_df.copy()\n",
    "test_df_domain = test_df.copy()\n",
    "\n",
    "train_df_domain['CREDIT_INCOME_PERCENT'] = train_df_domain['AMT_CREDIT'] / train_df_domain['AMT_INCOME_TOTAL']\n",
    "train_df_domain['ANNUITY_INCOME_PERCENT'] = train_df_domain['AMT_ANNUITY'] / train_df_domain['AMT_INCOME_TOTAL']\n",
    "train_df_domain['CREDIT_TERM'] = train_df_domain['AMT_ANNUITY'] / train_df_domain['AMT_CREDIT']\n",
    "train_df_domain['DAYS_EMPLOYED_PERCENT'] = train_df_domain['DAYS_EMPLOYED'] / train_df_domain['DAYS_BIRTH']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_df_domain['CREDIT_INCOME_PERCENT'] = test_df_domain['AMT_CREDIT'] / test_df_domain['AMT_INCOME_TOTAL']\n",
    "test_df_domain['ANNUITY_INCOME_PERCENT'] = test_df_domain['AMT_ANNUITY'] / test_df_domain['AMT_INCOME_TOTAL']\n",
    "test_df_domain['CREDIT_TERM'] = test_df_domain['AMT_ANNUITY'] / test_df_domain['AMT_CREDIT']\n",
    "test_df_domain['DAYS_EMPLOYED_PERCENT'] = test_df_domain['DAYS_EMPLOYED'] / test_df_domain['DAYS_BIRTH']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df['TARGET']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X = train_df_poly.drop(columns=['SK_ID_CURR'])\n",
    "y = train_df['TARGET']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "data = train_df_poly.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "test_df_poly = test_df_poly.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_df_poly = test_df_poly.drop(columns=['SK_ID_CURR'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X = data.drop(columns=['SK_ID_CURR'])\n",
    "y = train_df['TARGET']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lgb_train = lgb.Dataset(X_train,y_train)\n",
    "lgb_test = lgb.Dataset(X_test,y_test,reference=lgb_train)\n",
    "\n",
    "#scores = cross_val_score(rfc,X_train,y_train)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# LightGBMのパラメータ設定\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'num_leaves': 50,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'vervose': 0\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# ベストなパラメータ、途中経過を保存する\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    \"verbosity\": -1,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "}\n",
    "\n",
    "best_params, history = {}, []\n",
    "\n",
    "# LightGBM学習\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=100,\n",
    "                valid_sets=[lgb_train, lgb_test],\n",
    "                early_stopping_rounds=10\n",
    "               )\n",
    "\n",
    "best_params = gbm.params\n",
    "best_params"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predict1 = gbm.predict(test_df_poly)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "p.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pred1 = lgbcls.predict(X_test)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prediction = np.where(predict1 < 0.08,0,1)\n",
    "prediction"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predict = lgbcls.predict(test_df_poly)\n",
    "lgbcls.predict()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "submit = test_df[['SK_ID_CURR']]\n",
    "submit['TARGET'] = predict\n",
    "submit.to_csv('submit1.csv',index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "submit = test_df[['SK_ID_CURR']]\n",
    "submit['TARGET'] = prediction\n",
    "submit.to_csv('submit3.csv',index=False)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}